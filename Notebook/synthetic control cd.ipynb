{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35fb075c-bcc4-4a48-96e4-ef1e9ee506e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import warnings\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Deactivate all warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ccdbb65-7914-40f0-b70c-ea4a6f6d777e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.sql(\n",
    "    \"\"\"\n",
    "WITH daily_counts AS (\n",
    "    select\n",
    "        rt.route,\n",
    "        count(distinct rt.charge_dt) as observed_days\n",
    "    from \n",
    "        data_experience_commercial.cbt_1423_rtsuite.master rt\n",
    "    where 1=1 \n",
    "        and rt.chargeproduct = 'Ticket'\n",
    "        and rt.charge_dt between current_date() - 365 and current_date()\n",
    "    group by rt.route\n",
    ")\n",
    "select\n",
    "    rt.route,\n",
    "    rt.charge_dt,\n",
    "    rt.chargeproduct,\n",
    "\n",
    "    -- route characteristics\n",
    "    rtmap.type, \n",
    "    rtmap.region,\n",
    "\n",
    "    -- statistics & metrics\n",
    "    sum(flt.capacity) as capacity,\n",
    "    sum(rt.unt_net) as total_pax,\n",
    "    sum(rt.rev_net) as total_rev\n",
    "    \n",
    "from \n",
    "    data_experience_commercial.cbt_1423_rtsuite.master rt\n",
    "join \n",
    "    data_prod.silver_sanezdb.routemap rtmap on rt.route = rtmap.route\n",
    "join\n",
    "    data_prod.silver_curated_eres.flight flt on rt.flightkey = flt.flightkey\n",
    "join\n",
    "    daily_counts dc on rt.route = dc.route\n",
    "\n",
    "where 1=1 \n",
    "    and rt.chargeproduct = 'Ticket'\n",
    "    and rt.charge_dt between current_date() - 365 and current_date()\n",
    "    and dc.observed_days = 365  -- Ensures we have data for all days in the past year\n",
    "\n",
    "group by\n",
    "    rt.route,\n",
    "    rt.charge_dt,\n",
    "    rt.chargeproduct,\n",
    "    rtmap.type, \n",
    "    rtmap.region\n",
    "    \"\"\"\n",
    ").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5fa1d54-9f30-41a1-b0ee-7d01f79f5dc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def waterfall_sampling(df, strata, n, min_strata_size=5, seed=None):\n",
    "    \"\"\"\n",
    "    Perform stratified sampling using a waterfall approach.\n",
    "    \n",
    "    :param df: Pandas DataFrame containing the data.\n",
    "    :param strata: Tuple (region, type, cap_group) defining the stratum.\n",
    "    :param n: Number of samples to draw.\n",
    "    :param min_strata_size: Minimum size required for a stratum, otherwise 'region' is dropped.\n",
    "    :param seed: Random seed for reproducibility.\n",
    "    :return: Sampled DataFrame.\n",
    "    \"\"\"\n",
    "    region, rt_type, cap_group = strata\n",
    "\n",
    "    # First attempt: Full stratification (type, cap_group, region)\n",
    "    df_1st_level = df[(df[\"type\"] == rt_type) & \n",
    "                      (df[\"cap_group\"] == cap_group) & \n",
    "                      (df[\"region\"] == region)]\n",
    "\n",
    "    # If too small, drop 'region' and sample from (type, cap_group) level\n",
    "    if len(df_1st_level) < min_strata_size:\n",
    "        df_1st_level = df[(df[\"type\"] == rt_type) & \n",
    "                          (df[\"cap_group\"] == cap_group)]\n",
    "\n",
    "    # Ensure we don't sample more than available\n",
    "    sample_size = min(n, len(df_1st_level))\n",
    "\n",
    "    return df_1st_level.sample(n=sample_size, random_state=seed) if sample_size > 0 else pd.DataFrame()\n",
    "\n",
    "def repeat_waterfall_sampling(df, fixed_allocation, n_repeats=10):\n",
    "    \"\"\"\n",
    "    Perform waterfall sampling multiple times for all fixed allocation strata.\n",
    "    \n",
    "    :param df: Pandas DataFrame containing the data.\n",
    "    :param fixed_allocation: Dictionary with keys (region, type, cap_group) and values as sample sizes.\n",
    "    :param n_repeats: Number of times to repeat the sampling.\n",
    "    :return: List of sampled routes for each repeat.\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "\n",
    "    for _ in range(n_repeats):\n",
    "        seed = np.random.randint(1000)\n",
    "        sampled_routes = []\n",
    "\n",
    "        for strata, n in fixed_allocation.items():\n",
    "            sampled_df = waterfall_sampling(df, strata, n, seed=seed)\n",
    "            sampled_routes.extend(sampled_df.route.to_list())\n",
    "\n",
    "        samples.append(sampled_routes)\n",
    "\n",
    "    return samples\n",
    "\n",
    "def compute_sample_revenue(df, samples, target_routes):\n",
    "    \"\"\"\n",
    "    Compute the mean revenue per flight date for each sample.\n",
    "\n",
    "    :param df: DataFrame containing 'route', 'flight_dt', and 'total_rev'.\n",
    "    :param samples: List of lists, where each inner list contains sampled route names.\n",
    "    :param target_routes: Tuple or list of routes to compute the target revenue.\n",
    "    :return: DataFrame where each column represents a sample's mean revenue per flight date.\n",
    "    \"\"\"\n",
    "    rev_dict = {}\n",
    "\n",
    "    # Compute revenue for each sample\n",
    "    for idx, sample in enumerate(samples):\n",
    "        rev_dict[f'rev_sample{idx+1}'] = df[df['route'].isin(sample)].groupby('charge_dt')['total_rev'].mean()\n",
    "\n",
    "    # Compute target revenue if target_routes are provided\n",
    "    rev_dict['target'] = df[df['route'].isin(target_routes)].groupby('charge_dt')['total_rev'].mean()\n",
    "\n",
    "    return pd.DataFrame(rev_dict)\n",
    "\n",
    "def min_max_scale_df(df):\n",
    "    \"\"\"\n",
    "    Apply Min-Max scaling to each column of the given DataFrame.\n",
    "\n",
    "    :param df: DataFrame to scale.\n",
    "    :return: Min-Max scaled DataFrame.\n",
    "    \"\"\"\n",
    "    return (df - df.min()) / (df.max() - df.min())\n",
    "\n",
    "def plot_revenue_comparison(data):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    plt.plot(data.index, data['target'], label='Target', color='black', linewidth=2, linestyle='-', zorder=3)\n",
    "\n",
    "    for column in data.columns[:-1]:\n",
    "        plt.plot(data.index, data[column], label=column, alpha=0.4)\n",
    "\n",
    "    plt.title(\"Revenue Per Flight Date (Samples vs Target)\", fontsize=14)\n",
    "    plt.xlabel(\"Flight Date\", fontsize=12)\n",
    "    plt.ylabel(\"Mean Total Revenue\", fontsize=12)\n",
    "    plt.legend(title=\"Samples\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def compute_statistics(data, target_column='target', pred_column='counterfactual'):\n",
    "    \"\"\"\n",
    "    Compute bias and error in percentage terms without needing to descale.\n",
    "\n",
    "    :param data: DataFrame with scaled values between 0 and 1.\n",
    "    :param target_column: The column name for the target values in the DataFrame.\n",
    "    :return: Bias and Error in percentage.\n",
    "    \"\"\"\n",
    "\n",
    "    bias = 100 * (data[pred_column] - data[target_column]).mean()\n",
    "    error = 100 * np.abs(data[pred_column] - data[target_column]).mean()\n",
    "\n",
    "    return bias, error\n",
    "\n",
    "def plot_kde_results(insample_mape, insample_bias, test_mape, test_bias):\n",
    "    # Set the plot style\n",
    "    sns.set(style=\"whitegrid\")\n",
    "\n",
    "    # Create a figure with 4 subplots (1 row, 4 columns)\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "    # Plot each KDE on separate subplots\n",
    "    sns.kdeplot(insample_mape, label=\"Insample MAPE\", fill=True, alpha=0.6, ax=axes[0, 0])\n",
    "    axes[0, 0].set_title(\"Insample MAPE\", fontsize=14)\n",
    "    axes[0, 0].set_xlabel(\"Value\", fontsize=12)\n",
    "    axes[0, 0].set_ylabel(\"Density\", fontsize=12)\n",
    "\n",
    "    sns.kdeplot(insample_bias, label=\"Insample Bias\", fill=True, alpha=0.6, ax=axes[0, 1])\n",
    "    axes[0, 1].set_title(\"Insample Bias\", fontsize=14)\n",
    "    axes[0, 1].set_xlabel(\"Value\", fontsize=12)\n",
    "    axes[0, 1].set_ylabel(\"Density\", fontsize=12)\n",
    "\n",
    "    sns.kdeplot(test_mape, label=\"Test MAPE\", fill=True, alpha=0.6, ax=axes[1, 0])\n",
    "    axes[1, 0].set_title(\"Test MAPE\", fontsize=14)\n",
    "    axes[1, 0].set_xlabel(\"Value\", fontsize=12)\n",
    "    axes[1, 0].set_ylabel(\"Density\", fontsize=12)\n",
    "\n",
    "    sns.kdeplot(test_bias, label=\"Test Bias\", fill=True, alpha=0.6, ax=axes[1, 1])\n",
    "    axes[1, 1].set_title(\"Test Bias\", fontsize=14)\n",
    "    axes[1, 1].set_xlabel(\"Value\", fontsize=12)\n",
    "    axes[1, 1].set_ylabel(\"Density\", fontsize=12)\n",
    "\n",
    "    # Adjust layout to prevent overlapping\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92934126-fcfd-402a-99ae-482592ec44c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# A/A tests parameters \n",
    "df['charge_dt'] = pd.to_datetime(df['charge_dt']) # conversion datetime \n",
    "\n",
    "# create statistics df for stratified sampling\n",
    "df_stats = df.groupby(\n",
    "    ['route', 'region', 'type']\n",
    ").agg(\n",
    "    {\n",
    "        'capacity': 'sum',\n",
    "        'total_pax': 'mean',\n",
    "        'total_rev': 'mean'\n",
    "    }\n",
    ").reset_index()\n",
    "\n",
    "# Compute quantiles for each (region, type) group\n",
    "df_capacity_quantiles = df_stats.groupby(['region', 'type']).agg(\n",
    "    capacity_25th=('capacity', lambda x: x.quantile(0.25)),\n",
    "    capacity_50th=('capacity', lambda x: x.quantile(0.50)),\n",
    "    capacity_75th=('capacity', lambda x: x.quantile(0.75))\n",
    ").reset_index()\n",
    "\n",
    "# Merge back to the original df_stats\n",
    "df_stats = df_stats.merge(df_capacity_quantiles, on=['region', 'type'], how='left')\n",
    "\n",
    "def cap_group(row):\n",
    "    if row['capacity'] <= row['capacity_50th']:\n",
    "        return 'Low'\n",
    "    return 'High'\n",
    "df_stats['cap_group'] = df_stats.apply(cap_group, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f7f4516-63ca-4adb-8374-93a49e63cac5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Test period = 90 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef4de470-aa9f-43ae-a65d-3c16e2e2ac3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# pre loop stuff\n",
    "n_sim = 500\n",
    "test_period = 90\n",
    "n_markets = 50\n",
    "insample_mape = []\n",
    "insaple_bias = []\n",
    "test_mape = []\n",
    "test_bias = []\n",
    "estimated_lift = []\n",
    "\n",
    "for _ in range(n_sim):\n",
    "    # get test routes and fixed allocations \n",
    "    test_routes = np.random.choice(df_stats.route.unique(), size=n_markets, replace=False)\n",
    "    fixed_allocation = df_stats.loc[\n",
    "        df_stats.route.isin(test_routes)\n",
    "    ].groupby(['region', 'type', 'cap_group']).size().to_dict()\n",
    "\n",
    "    # remove markets so as not to sample from it\n",
    "    df_stats_temp = df_stats.loc[\n",
    "        ~df_stats.route.isin(test_routes)\n",
    "    ]\n",
    "    # get stratified samples \n",
    "    samples = repeat_waterfall_sampling(df_stats_temp, fixed_allocation, n_repeats=n_markets)\n",
    "\n",
    "    # create dataset\n",
    "    data = compute_sample_revenue(df, samples, test_routes)\n",
    "    data = min_max_scale_df(data)\n",
    "    data = data.dropna()\n",
    "    #plot_revenue_comparison(data)\n",
    "\n",
    "    train, test = data.iloc[:test_period], data.iloc[test_period:]\n",
    "    X = train.drop(columns=['target'])\n",
    "    y = train['target']\n",
    "    reg = LinearRegression().fit(X, y)\n",
    "\n",
    "    train['counterfactual'] = reg.predict(train.drop(columns=['target']))\n",
    "    test['counterfactual'] = reg.predict(test.drop(columns=['target']))\n",
    "    biais, error = compute_statistics(train)\n",
    "    insample_mape.append(error)\n",
    "    insaple_bias.append(biais)\n",
    "    biais, error = compute_statistics(test)\n",
    "    test_mape.append(error)\n",
    "    test_bias.append(biais)\n",
    "\n",
    "    # impact 2% that is stochastic \n",
    "    lift_shock = np.random.normal(loc=0.02, scale=0.05, size=len(test))\n",
    "    lift = 100 * ((test['counterfactual'] + lift_shock).mean() - test['target'].mean())\n",
    "    estimated_lift.append(lift)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4be6d0d-7ee2-4072-8a1e-3e2a80cc9c9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_kde_results(\n",
    "    insample_mape,\n",
    "    insaple_bias,\n",
    "    test_mape,\n",
    "    test_bias,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9e88cdf-c968-4fa2-affb-c7da5f7d278d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " sns.kdeplot(estimated_lift, label=\"Estimated Impact \", fill=True, alpha=0.6)\n",
    " plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "890fc5f2-c3a3-4245-9c5e-172a6ecfe5bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tresh_05, tresh_1, tresh_2 = np.quantile(estimated_lift, [0.05, 0.1, 0.2])\n",
    "print('Lower Bound at 5%: ', tresh_05)\n",
    "print('Lower Bound at 10%: ', tresh_1)\n",
    "print('Lower Bound at 20%: ', tresh_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4d47d92-b8fa-42b9-a4d3-3fc19f16d70e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Test period = 60 days "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "318e405a-1ab6-4c5f-8c64-67a6ec8e5a30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# pre loop stuff\n",
    "n_sim = 500\n",
    "test_period = 60\n",
    "n_markets = 50\n",
    "insample_mape = []\n",
    "insaple_bias = []\n",
    "test_mape = []\n",
    "test_bias = []\n",
    "estimated_lift = []\n",
    "\n",
    "for _ in range(n_sim):\n",
    "    # get test routes and fixed allocations \n",
    "    test_routes = np.random.choice(df_stats.route.unique(), size=n_markets, replace=False)\n",
    "    fixed_allocation = df_stats.loc[\n",
    "        df_stats.route.isin(test_routes)\n",
    "    ].groupby(['region', 'type', 'cap_group']).size().to_dict()\n",
    "\n",
    "    # remove markets so as not to sample from it\n",
    "    df_stats_temp = df_stats.loc[\n",
    "        ~df_stats.route.isin(test_routes)\n",
    "    ]\n",
    "    # get stratified samples \n",
    "    samples = repeat_waterfall_sampling(df_stats_temp, fixed_allocation, n_repeats=n_markets)\n",
    "\n",
    "    # create dataset\n",
    "    data = compute_sample_revenue(df, samples, test_routes)\n",
    "    data = min_max_scale_df(data)\n",
    "    data = data.dropna()\n",
    "    #plot_revenue_comparison(data)\n",
    "\n",
    "    train, test = data.iloc[:test_period], data.iloc[test_period:]\n",
    "    X = train.drop(columns=['target'])\n",
    "    y = train['target']\n",
    "    reg = LinearRegression().fit(X, y)\n",
    "\n",
    "    train['counterfactual'] = reg.predict(train.drop(columns=['target']))\n",
    "    test['counterfactual'] = reg.predict(test.drop(columns=['target']))\n",
    "    biais, error = compute_statistics(train)\n",
    "    insample_mape.append(error)\n",
    "    insaple_bias.append(biais)\n",
    "    biais, error = compute_statistics(test)\n",
    "    test_mape.append(error)\n",
    "    test_bias.append(biais)\n",
    "\n",
    "    # impact 2% that is stochastic \n",
    "    lift_shock = np.random.normal(loc=0.02, scale=0.05, size=len(test))\n",
    "    lift = 100 * ((test['counterfactual'] + lift_shock).mean() - test['target'].mean())\n",
    "    estimated_lift.append(lift)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18a82f79-ce64-490a-92e9-2317a3beede6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_kde_results(\n",
    "    insample_mape,\n",
    "    insaple_bias,\n",
    "    test_mape,\n",
    "    test_bias,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6377754-4ef9-4078-af04-608bcaa53bbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tresh_05, tresh_1, tresh_2 = np.quantile(estimated_lift, [0.05, 0.1, 0.2])\n",
    "print('Lower Bound at 5%: ', tresh_05)\n",
    "print('Lower Bound at 10%: ', tresh_1)\n",
    "print('Lower Bound at 20%: ', tresh_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e9257ba-af24-4f4a-99ad-0b4822555a9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Test period = 30 days "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd9d39e1-cb18-46c5-8f42-33433685df9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# pre loop stuff\n",
    "n_sim = 500\n",
    "test_period = 30\n",
    "n_markets = 50\n",
    "insample_mape = []\n",
    "insaple_bias = []\n",
    "test_mape = []\n",
    "test_bias = []\n",
    "estimated_lift = []\n",
    "\n",
    "for _ in range(n_sim):\n",
    "    # get test routes and fixed allocations \n",
    "    test_routes = np.random.choice(df_stats.route.unique(), size=n_markets, replace=False)\n",
    "    fixed_allocation = df_stats.loc[\n",
    "        df_stats.route.isin(test_routes)\n",
    "    ].groupby(['region', 'type', 'cap_group']).size().to_dict()\n",
    "\n",
    "    # remove markets so as not to sample from it\n",
    "    df_stats_temp = df_stats.loc[\n",
    "        ~df_stats.route.isin(test_routes)\n",
    "    ]\n",
    "    # get stratified samples \n",
    "    samples = repeat_waterfall_sampling(df_stats_temp, fixed_allocation, n_repeats=n_markets)\n",
    "\n",
    "    # create dataset\n",
    "    data = compute_sample_revenue(df, samples, test_routes)\n",
    "    data = min_max_scale_df(data)\n",
    "    data = data.dropna()\n",
    "    #plot_revenue_comparison(data)\n",
    "\n",
    "    train, test = data.iloc[:test_period], data.iloc[test_period:]\n",
    "    X = train.drop(columns=['target'])\n",
    "    y = train['target']\n",
    "    reg = LinearRegression().fit(X, y)\n",
    "\n",
    "    train['counterfactual'] = reg.predict(train.drop(columns=['target']))\n",
    "    test['counterfactual'] = reg.predict(test.drop(columns=['target']))\n",
    "    biais, error = compute_statistics(train)\n",
    "    insample_mape.append(error)\n",
    "    insaple_bias.append(biais)\n",
    "    biais, error = compute_statistics(test)\n",
    "    test_mape.append(error)\n",
    "    test_bias.append(biais)\n",
    "\n",
    "    # impact 2% that is stochastic \n",
    "    lift_shock = np.random.normal(loc=0.02, scale=0.05, size=len(test))\n",
    "    lift = 100 * ((test['counterfactual'] + lift_shock).mean() - test['target'].mean())\n",
    "    estimated_lift.append(lift)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b08ad60-32ca-4c4b-97e1-e67aea339b66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_kde_results(\n",
    "    insample_mape,\n",
    "    insaple_bias,\n",
    "    test_mape,\n",
    "    test_bias,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90f44a8e-6505-4c90-a861-0990a3a78678",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tresh_05, tresh_1, tresh_2 = np.quantile(estimated_lift, [0.05, 0.1, 0.2])\n",
    "print('Lower Bound at 5%: ', tresh_05)\n",
    "print('Lower Bound at 10%: ', tresh_1)\n",
    "print('Lower Bound at 20%: ', tresh_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "648ff599-cfeb-4675-9d3d-c1b83c896103",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "synthetic control cd",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
