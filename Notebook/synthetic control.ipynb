{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc254859-8933-4f9e-bf8d-12bcd8b246ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import warnings\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Deactivate all warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fca68acf-3ca8-452d-807e-e2c6d39ecfe8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.sql(\n",
    "    \"\"\"\n",
    "    select\n",
    "        rt.flightkey,\n",
    "        rt.route,\n",
    "        rt.flight_dt,\n",
    "        rt.chargeproduct,\n",
    "        flt.capacity,\n",
    "\n",
    "        -- route characteristics\n",
    "        rtmap.type, \n",
    "        rtmap.region,\n",
    "\n",
    "        -- statistics & metrics\n",
    "        sum(rt.unt_net) as total_pax,\n",
    "        sum(rt.rev_net) as total_rev\n",
    "        \n",
    "    from \n",
    "        data_experience_commercial.cbt_1423_rtsuite.master rt\n",
    "    join \n",
    "        data_prod.silver_sanezdb.routemap rtmap on rt.route = rtmap.route\n",
    "    join\n",
    "        data_prod.silver_curated_eres.flight flt on rt.flightkey = flt.flightkey\n",
    "\n",
    "    where 1=1 \n",
    "        and rt.chargeproduct = 'Ticket'\n",
    "        and rt.flight_dt between current_date() - 365 and current_date()\n",
    "\n",
    "    group by\n",
    "        rt.flightkey,\n",
    "        rt.route,\n",
    "        rt.flight_dt,\n",
    "        rt.chargeproduct,\n",
    "        flt.capacity, \n",
    "        rtmap.type, \n",
    "        rtmap.region\n",
    "    \"\"\"\n",
    ").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca681190-2a50-4f7b-941f-10ae15dd73c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "224227d3-ff9d-48f4-954c-5fde4d1ace5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def waterfall_sampling(df, strata, n, min_strata_size=5, seed=None):\n",
    "    \"\"\"\n",
    "    Perform stratified sampling using a waterfall approach.\n",
    "    \n",
    "    :param df: Pandas DataFrame containing the data.\n",
    "    :param strata: Tuple (region, type, cap_group) defining the stratum.\n",
    "    :param n: Number of samples to draw.\n",
    "    :param min_strata_size: Minimum size required for a stratum, otherwise 'region' is dropped.\n",
    "    :param seed: Random seed for reproducibility.\n",
    "    :return: Sampled DataFrame.\n",
    "    \"\"\"\n",
    "    region, rt_type, cap_group = strata\n",
    "\n",
    "    # First attempt: Full stratification (type, cap_group, region)\n",
    "    df_1st_level = df[(df[\"type\"] == rt_type) & \n",
    "                      (df[\"cap_group\"] == cap_group) & \n",
    "                      (df[\"region\"] == region)]\n",
    "\n",
    "    # If too small, drop 'region' and sample from (type, cap_group) level\n",
    "    if len(df_1st_level) < min_strata_size:\n",
    "        df_1st_level = df[(df[\"type\"] == rt_type) & \n",
    "                          (df[\"cap_group\"] == cap_group)]\n",
    "\n",
    "    # Ensure we don't sample more than available\n",
    "    sample_size = min(n, len(df_1st_level))\n",
    "\n",
    "    return df_1st_level.sample(n=sample_size, random_state=seed) if sample_size > 0 else pd.DataFrame()\n",
    "\n",
    "def repeat_waterfall_sampling(df, fixed_allocation, n_repeats=10):\n",
    "    \"\"\"\n",
    "    Perform waterfall sampling multiple times for all fixed allocation strata.\n",
    "    \n",
    "    :param df: Pandas DataFrame containing the data.\n",
    "    :param fixed_allocation: Dictionary with keys (region, type, cap_group) and values as sample sizes.\n",
    "    :param n_repeats: Number of times to repeat the sampling.\n",
    "    :return: List of sampled routes for each repeat.\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "\n",
    "    for _ in range(n_repeats):\n",
    "        seed = np.random.randint(1000)\n",
    "        sampled_routes = []\n",
    "\n",
    "        for strata, n in fixed_allocation.items():\n",
    "            sampled_df = waterfall_sampling(df, strata, n, seed=seed)\n",
    "            sampled_routes.extend(sampled_df.route.to_list())\n",
    "\n",
    "        samples.append(sampled_routes)\n",
    "\n",
    "    return samples\n",
    "\n",
    "def compute_sample_revenue(df, samples, target_routes):\n",
    "    \"\"\"\n",
    "    Compute the mean revenue per flight date for each sample.\n",
    "\n",
    "    :param df: DataFrame containing 'route', 'flight_dt', and 'total_rev'.\n",
    "    :param samples: List of lists, where each inner list contains sampled route names.\n",
    "    :param target_routes: Tuple or list of routes to compute the target revenue.\n",
    "    :return: DataFrame where each column represents a sample's mean revenue per flight date.\n",
    "    \"\"\"\n",
    "    rev_dict = {}\n",
    "\n",
    "    # Compute revenue for each sample\n",
    "    for idx, sample in enumerate(samples):\n",
    "        rev_dict[f'rev_sample{idx+1}'] = df[df['route'].isin(sample)].groupby('flight_dt')['total_rev'].mean()\n",
    "\n",
    "    # Compute target revenue if target_routes are provided\n",
    "    rev_dict['target'] = df[df['route'].isin(target_routes)].groupby('flight_dt')['total_rev'].mean()\n",
    "\n",
    "    return pd.DataFrame(rev_dict)\n",
    "\n",
    "def min_max_scale_df(df):\n",
    "    \"\"\"\n",
    "    Apply Min-Max scaling to each column of the given DataFrame.\n",
    "\n",
    "    :param df: DataFrame to scale.\n",
    "    :return: Min-Max scaled DataFrame.\n",
    "    \"\"\"\n",
    "    return (df - df.min()) / (df.max() - df.min())\n",
    "\n",
    "def plot_revenue_comparison(data):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    plt.plot(data.index, data['target'], label='Target', color='black', linewidth=2, linestyle='-', zorder=3)\n",
    "\n",
    "    for column in data.columns[:-1]:\n",
    "        plt.plot(data.index, data[column], label=column, alpha=0.4)\n",
    "\n",
    "    plt.title(\"Revenue Per Flight Date (Samples vs Target)\", fontsize=14)\n",
    "    plt.xlabel(\"Flight Date\", fontsize=12)\n",
    "    plt.ylabel(\"Mean Total Revenue\", fontsize=12)\n",
    "    plt.legend(title=\"Samples\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def compute_statistics(data, target_column='target', pred_column='counterfactual'):\n",
    "    \"\"\"\n",
    "    Compute bias and error in percentage terms without needing to descale.\n",
    "\n",
    "    :param data: DataFrame with scaled values between 0 and 1.\n",
    "    :param target_column: The column name for the target values in the DataFrame.\n",
    "    :return: Bias and Error in percentage.\n",
    "    \"\"\"\n",
    "\n",
    "    bias = 100 * (data[pred_column] - data[target_column]).mean()\n",
    "    error = 100 * np.abs(data[pred_column] - data[target_column]).mean()\n",
    "    \n",
    "    return bias, error\n",
    "\n",
    "def plot_kde_results(insample_mape, insample_bias, test_mape, test_bias):\n",
    "    # Set the plot style\n",
    "    sns.set(style=\"whitegrid\")\n",
    "\n",
    "    # Create a figure with 4 subplots (1 row, 4 columns)\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "    # Plot each KDE on separate subplots\n",
    "    sns.kdeplot(insample_mape, label=\"Insample MAPE\", fill=True, alpha=0.6, ax=axes[0, 0])\n",
    "    axes[0, 0].set_title(\"Insample MAPE\", fontsize=14)\n",
    "    axes[0, 0].set_xlabel(\"Value\", fontsize=12)\n",
    "    axes[0, 0].set_ylabel(\"Density\", fontsize=12)\n",
    "\n",
    "    sns.kdeplot(insample_bias, label=\"Insample Bias\", fill=True, alpha=0.6, ax=axes[0, 1])\n",
    "    axes[0, 1].set_title(\"Insample Bias\", fontsize=14)\n",
    "    axes[0, 1].set_xlabel(\"Value\", fontsize=12)\n",
    "    axes[0, 1].set_ylabel(\"Density\", fontsize=12)\n",
    "\n",
    "    sns.kdeplot(test_mape, label=\"Test MAPE\", fill=True, alpha=0.6, ax=axes[1, 0])\n",
    "    axes[1, 0].set_title(\"Test MAPE\", fontsize=14)\n",
    "    axes[1, 0].set_xlabel(\"Value\", fontsize=12)\n",
    "    axes[1, 0].set_ylabel(\"Density\", fontsize=12)\n",
    "\n",
    "    sns.kdeplot(test_bias, label=\"Test Bias\", fill=True, alpha=0.6, ax=axes[1, 1])\n",
    "    axes[1, 1].set_title(\"Test Bias\", fontsize=14)\n",
    "    axes[1, 1].set_xlabel(\"Value\", fontsize=12)\n",
    "    axes[1, 1].set_ylabel(\"Density\", fontsize=12)\n",
    "\n",
    "    # Adjust layout to prevent overlapping\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ce75ddd-2720-408e-b3e0-4941820964b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## A/A tests synthetic control\n",
    "\n",
    "Goal: find out the level of accuracy that will yield synthetic control for A/B tests when the number of route is limited (e.g. 15) in s2 settings. \n",
    "Methodology: We will run many simulations. In the first stage, we generate a test set random sampling. In a second stage we generate many control groups which have about the same amount of route through stratified sampling. Then we fit pre-test the stratified contol / test and compute the RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c12c0465-d5b0-4076-a8f7-d341bdb8d9ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# A/A tests parameters \n",
    "df['fligth_dt'] = pd.to_datetime(df['flight_dt']) # conversion datetime \n",
    "\n",
    "# create statistics df for stratified sampling\n",
    "df_stats = df_stats = df_test.groupby(\n",
    "    ['route', 'region', 'type']\n",
    ").agg(\n",
    "    {\n",
    "        'capacity': 'sum',\n",
    "        'total_pax': 'mean',\n",
    "        'total_rev': 'mean'\n",
    "    }\n",
    ").reset_index()\n",
    "\n",
    "# Compute quantiles for each (region, type) group\n",
    "df_capacity_quantiles = df_stats.groupby(['region', 'type']).agg(\n",
    "    capacity_25th=('capacity', lambda x: x.quantile(0.25)),\n",
    "    capacity_50th=('capacity', lambda x: x.quantile(0.50)),\n",
    "    capacity_75th=('capacity', lambda x: x.quantile(0.75))\n",
    ").reset_index()\n",
    "\n",
    "# Merge back to the original df_stats\n",
    "df_stats = df_stats.merge(df_capacity_quantiles, on=['region', 'type'], how='left')\n",
    "\n",
    "def cap_group(row):\n",
    "    if row['capacity'] <= row['capacity_50th']:\n",
    "        return 'Low'\n",
    "    return 'High'\n",
    "df_stats['cap_group'] = df_stats.apply(cap_group, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab0cee67-ebaf-41ce-91ee-9531a3e9a113",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# pre loop stuff\n",
    "n_sim = 1_000\n",
    "test_period = 90\n",
    "insample_mape = []\n",
    "insaple_bias = []\n",
    "test_mape = []\n",
    "test_bias = []\n",
    "\n",
    "for _ in range(n_sim):\n",
    "    # get test routes and fixed allocations \n",
    "    test_routes = np.random.choice(df_stats.route.unique(), size=15, replace=False)\n",
    "    fixed_allocation = df_stats.loc[\n",
    "        df_stats.route.isin(test_routes)\n",
    "    ].groupby(['region', 'type', 'cap_group']).size().to_dict()\n",
    "\n",
    "    # remove markets so as not to sample from it\n",
    "    df_stats_temp = df_stats.loc[\n",
    "        ~df_stats.route.isin(test_routes)\n",
    "    ]\n",
    "    # get stratified samples \n",
    "    samples = repeat_waterfall_sampling(df_stats_temp, fixed_allocation, n_repeats=10)\n",
    "\n",
    "    # create dataset\n",
    "    data = compute_sample_revenue(df, samples, test_routes)\n",
    "    data = min_max_scale_df(data)\n",
    "    data = data.dropna()\n",
    "    #plot_revenue_comparison(data)\n",
    "\n",
    "    train, test = data.iloc[:test_period], data.iloc[test_period:]\n",
    "    X = train.drop(columns=['target'])\n",
    "    y = train['target']\n",
    "    reg = LinearRegression().fit(X, y)\n",
    "\n",
    "    train['counterfactual'] = reg.predict(train.drop(columns=['target']))\n",
    "    test['counterfactual'] = reg.predict(test.drop(columns=['target']))\n",
    "    biais, error = compute_statistics(train)\n",
    "    insample_mape.append(error)\n",
    "    insaple_bias.append(biais)\n",
    "    biais, error = compute_statistics(test)\n",
    "    test_mape.append(error)\n",
    "    test_bias.append(biais)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38196618-b053-4a9a-a9ce-fd0ce03beada",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_kde_results(\n",
    "    insample_mape,\n",
    "    insaple_bias,\n",
    "    test_mape,\n",
    "    test_bias,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "synthetic control",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
